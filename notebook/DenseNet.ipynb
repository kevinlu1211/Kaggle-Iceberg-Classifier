{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some initial preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../data/train.json\"\n",
    "data = pd.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band1_rs'] = data['band_1'].map(lambda x: np.reshape(np.array(x), (75, 75)))\n",
    "data['band2_rs'] = data['band_2'].map(lambda x: np.reshape(np.array(x), (75, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_img = np.stack([data['band1_rs'].tolist(), data['band2_rs'].tolist()],1)\n",
    "labels = np.expand_dims(data['is_iceberg'].tolist(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(full_img, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, n_input_features, k, bn_size, drop_prob, bias=False):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.add_module('bn1', nn.BatchNorm2d(n_input_features))\n",
    "        self.add_module('relu1', nn.ReLU())\n",
    "        self.add_module('conv.1', nn.Conv2d(n_input_features, k * bn_size, \n",
    "          kernel_size=1, stride=1, bias=bias))\n",
    "        self.add_module('bn2', nn.BatchNorm2d(k * bn_size))\n",
    "        self.add_module('relu2', nn.ReLU())\n",
    "        self.add_module('conv2', nn.Conv2d(k * bn_size, k, \n",
    "          kernel_size=3, stride=1, padding=1, bias=bias))\n",
    "                        \n",
    "    def forward(self, inp):  \n",
    "        output = super(DenseLayer, self).forward(inp)\n",
    "        if self.drop_prob > 0:\n",
    "            output = F.dropout(output, p=self.drop_prob, training=self.training)\n",
    "        return torch.cat([inp, output], 1)\n",
    "\n",
    "class DenseBlock(nn.Sequential):\n",
    "    def __init__(self, n_layers, n_init_features, k, bn_size, drop_prob, bias=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            layer = DenseLayer(n_init_features + i * k, k, \n",
    "                               bn_size, drop_prob, bias)\n",
    "            self.add_module(f'denselayer{i}', layer)\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, n_input_features, n_output_features): \n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.add_module('bn', nn.BatchNorm2d(n_input_features))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('conv', nn.Conv2d(n_input_features, n_output_features, \n",
    "                      kernel_size=1, stride=1, padding=(1,1)))\n",
    "        self.add_module('avgpool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate = 16, block_config=(6, 12, 24, 16), n_init_features=32,\n",
    "                 bn_size=4, drop_prob=0, n_classes=1, input_shape=(2, 75, 75)):\n",
    "        super(DenseNet, self).__init__() \n",
    " \n",
    "        # Input layer conv\n",
    "        self.features = nn.Sequential(\n",
    "             OrderedDict([\n",
    "                 ('conv0', nn.Conv2d(2, n_init_features, kernel_size=5, stride=1))\n",
    "             ])\n",
    "        )\n",
    "        n_features = n_init_features\n",
    "        for i, n_layers in enumerate(block_config):\n",
    "            block = DenseBlock(n_layers, n_features, growth_rate, bn_size, drop_prob)\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            n_features = n_features + growth_rate * n_layers\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = TransitionLayer(n_input_features=n_features, n_output_features=n_features//2)\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                n_features = n_features // 2\n",
    "                \n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(n_features)) \n",
    "        n_fc = self._get_conv_output(input_shape)\n",
    "        self.classifier = nn.Linear(n_fc, n_classes)\n",
    "    \n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        inp = Variable(torch.rand(bs, *shape))\n",
    "        output_features = self.features(inp)\n",
    "        n_features = output_features.data.view(bs, -1).size(1)\n",
    "        return n_features\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        output = self.features(inp)\n",
    "        output = output.view(output.size(0), -1) \n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.00005\n",
    "loss_criterion = nn.BCELoss()\n",
    "model = DenseNet(block_config=(2, 4, 2), drop_prob=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-5)  # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    losses_for_epoch = []\n",
    "    train_X, train_y = shuffle(train_X, train_y)\n",
    "    for j in tqdm(range(0, len(train_X), batch_size)):\n",
    "        batch_X = train_X[j:j+batch_size]\n",
    "        batch_X = Variable(torch.from_numpy(batch_X).float()) \n",
    "        batch_y = train_y[j:j+batch_size] \n",
    "        batch_y = Variable(torch.from_numpy(batch_y).float())\n",
    "        \n",
    "        out = model(batch_X)\n",
    "        out = F.sigmoid(out)\n",
    "        loss = loss_criterion(out, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_for_epoch.append(loss.data[0])\n",
    "        \n",
    "    avg_loss_for_epoch = sum(losses_for_epoch)/len(losses_for_epoch)\n",
    "    all_losses.append(avg_loss_for_epoch)\n",
    "    print(f\"Finish {epoch+1} epoch, Average Loss: {avg_loss_for_epoch:.3f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    eval_losses_for_epoch = []\n",
    "    for j in tqdm(range(0, len(val_X), batch_size)):\n",
    "        batch_X = val_X[j:j+batch_size]\n",
    "        batch_X = Variable(torch.from_numpy(batch_X).float()) \n",
    "        batch_y = val_y[j:j+batch_size] \n",
    "        batch_y = Variable(torch.from_numpy(batch_y).float())\n",
    "        \n",
    "        out = model(batch_X)\n",
    "        out = F.sigmoid(out)\n",
    "        loss = loss_criterion(out, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        eval_losses_for_epoch.append(loss.data[0])\n",
    "        \n",
    "    avg_eval_loss_for_epoch = sum(eval_losses_for_epoch)/len(eval_losses_for_epoch)\n",
    "    print(f\"VALIDATION Loss: {avg_eval_loss_for_epoch:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
